{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9d7e-8f48-41e9-94ab-5c3b1b717694",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "## Домашнее задание 4: Direct Preference Optimization \n",
    "\n",
    "__Мягкий дедлайн 16.11.25 23:59__ \\\n",
    "__Жесткий дедлайн 19.11.25 23:59__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит обучить большую LLM для ответов на вопросы с помощью DPO, а также реализовать LoRA для эффективного обучения. \n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — __11 баллов__.\n",
    "\n",
    "Оценка за это домашнее задание будет формироваться из оценки за __задания__ и за __отчет__, в котором от вас требуется написать о проделанной работе. За отчет можно получить до 2-х баллов, однако в случае отсутствия отчета баллы за соответствующие задания не будут ставиться. Мы настаиваем на том, чтобы вы оформили весь код в виде полноценного проекта. Этот ноутбук нужно рассматривать скорее как файл с условием, чем как место для написания массивного кода. За сдачу больших ноутбуков с кодом оценка будет снижена. Ответы на все вопросы в заданиях можно (нужно) писать в отчете.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "### План решения\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*lK6iJMz5CGh2fo7TsDn15A.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Обучение следованию инструкциям с помощью DPO разбивается на два этапа:    \n",
    "1. __Supervised Fine-tuning (SFT)__ – обучение базовой модели ответам на запросы в нужном формате.\n",
    "2. __Direct Preference Optimization (DPO)__ – обучение SFT модели приоритизации \"хороших\" ответов.\n",
    "\n",
    "Мы не хотим обучать модели целиком по двум причинам: 1) используемые модели очень большие; 2) нам требуется лишь выравнить модель с нашими предпочтениями, не внося в нее новых знаний, что не требует серьезного обучения. Поэтому мы будем использовать PEFT, а именно LoRA для обучения.\n",
    "\n",
    "Таким образом, вам надо будет:\n",
    "1. Реализовать и протестировать LoRA\n",
    "2. Разобраться с данными и привести их к нужному формату\n",
    "3. Обучить SFT модель\n",
    "4. Обучить DPO модель\n",
    "5. Порадоваться, что вы молодцы и со всем справились\n",
    "6. (Опционально) сделать веб-интерфейс для вашей модели, переиспользуя код из первой домашки (мы можем выдать бонусы, если получится классно)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe17e5-2099-48d6-8215-2eeed2d07f82",
   "metadata": {},
   "source": [
    "### О датасете\n",
    "\n",
    "Мы будем работать с датасетом [Anthropic Helpful-Harmless](https://huggingface.co/datasets/Anthropic/hh-rlhf) для RLHF. В нем содержится 160к примеров ответов на вопросы с историей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e484eae-0bd1-439d-8ffa-6230dbb84c30",
   "metadata": {},
   "source": [
    "### Low-Rank Adaptation (LoRA)\n",
    "\n",
    "<img src=\"https://heidloff.net/assets/img/2023/08/lora.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "__Задание 1 (3 балла).__ Реализуйте самостоятельно модуль LoRA для эффективного обучения LLM по схеме, описанной в [статье](https://arxiv.org/pdf/2106.09685). Встройте его в свою любимую LLM и убедитесь, что ошибка убывает при обучении параметров LoRA на безусловную генерацию. Для этого возьмите любые данные на свой выбор. Замерьте насколько уменьшилось число обучаемых параметров, как изменилась скорость во время forward и backward процессов и как изменились затраты по памяти. Сделайте выводы и напишите о них в отчете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51517498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinfillo/Desktop/UniHW/NLP/HW4/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1639dac-cf3c-4312-8330-d4f357f38c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Обёртка над nn.Linear с добавкой низкорангового сдвига BAx.\n",
    "    Базовый слой W заморожен, обучаем только A и B.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, r, alpha, dropout, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.base = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.lora_A = nn.Linear(in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, out_features, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.base.reset_parameters()\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.base(x)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        return base_out + lora_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9dfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_base_and_unfreeze_lora(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"lora_A\" in name or \"lora_B\" in name:\n",
    "            p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4bf6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b39a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_module(model, module_name):\n",
    "    parts = module_name.split(\".\")\n",
    "    parent = model\n",
    "    for p in parts[:-1]:\n",
    "        parent = getattr(parent, p)\n",
    "    return parent, parts[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb84e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model, r=8, alpha=16, dropout=0.05, target=(\"q_proj\", \"v_proj\")):\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear) and any(t in name for t in target):\n",
    "            parent, attr = get_parent_module(model, name)\n",
    "\n",
    "            lora = LoRA(module.in_features, module.out_features, r=r, alpha=alpha, dropout=dropout, bias=(module.bias is not None))\n",
    "            lora.base.weight.data = module.weight.data.clone()\n",
    "            if module.bias is not None:\n",
    "                lora.base.bias.data = module.bias.data.clone()\n",
    "\n",
    "            setattr(parent, attr, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0231e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLMDataset(Dataset):\n",
    "    def __init__(self, token_ids, block_size):\n",
    "        self.data = token_ids\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - 1) // self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.block_size\n",
    "        end = start + self.block_size\n",
    "        x = self.data[start:end]\n",
    "        y = self.data[start + 1:end + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c24da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca68392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 331762.32 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1779495.69 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1094039.75 examples/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2518423 > 32768). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "block_size = 128\n",
    "batch_size = 8\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_text = \"\\n\\n\".join(raw[\"train\"][\"text\"])\n",
    "valid_text = \"\\n\\n\".join(raw[\"validation\"][\"text\"])\n",
    "\n",
    "train_ids = tokenizer(train_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].squeeze(0)\n",
    "valid_ids = tokenizer(valid_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].squeeze(0)\n",
    "\n",
    "train_ds = CausalLMDataset(train_ids, block_size)\n",
    "valid_ds = CausalLMDataset(valid_ids, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15c8b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_lm_loss(model, dataloader, device):\n",
    "    model.eval()\n",
    "    accum_loss = 0.0\n",
    "    n_tokens = 0\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(input_ids=x, labels=y)\n",
    "        loss = out.loss\n",
    "        B, T = x.shape\n",
    "        accum_loss += loss.item() * (B * T)\n",
    "        n_tokens += B * T\n",
    "    return accum_loss / n_tokens\n",
    "\n",
    "def train_epoch_lm(model, dataloader, optimizer, device, clip=1.0):\n",
    "    model.train()\n",
    "    accum_loss = 0.0\n",
    "    n_tokens = 0\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out = model(input_ids=x, labels=y)\n",
    "        loss = out.loss\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        B, T = x.shape\n",
    "        accum_loss += loss.item() * (B * T)\n",
    "        n_tokens += B * T\n",
    "\n",
    "    return accum_loss / n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3af638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, train_loader, valid_loader, device, lr=1e-4, epochs=1, clip=1.0, desc=\"\"):\n",
    "    model.to(device)\n",
    "    total, trainable = count_parameters(model)\n",
    "\n",
    "    print(f\"\\n=== {desc} ===\")\n",
    "    print(f\"Всего параметров:     {total/1e6:.2f}M\")\n",
    "    print(f\"Обучаемых параметров: {trainable/1e6:.2f}M \" f\"({100*trainable/total:.3f}%)\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(1, epаochs + 1):\n",
    "        if device.startswith(\"cuda\"):\n",
    "            torch.cuda.reset_peak_memory_stats(device)\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        train_loss = train_epoch_lm(model, train_loader, optimizer, device, clip)\n",
    "        val_loss = evaluate_lm_loss(model, valid_loader, device)\n",
    "\n",
    "        elapsed = time.perf_counter() - start\n",
    "        if device.startswith(\"cuda\"):\n",
    "            max_mem = torch.cuda.max_memory_allocated(device) / 1024**2\n",
    "        else:\n",
    "            max_mem = float(\"nan\")\n",
    "\n",
    "        history.append((train_loss, val_loss, elapsed, max_mem))\n",
    "\n",
    "        print(f\"epoch {epoch}: train_loss={train_loss:.4f}, \"\n",
    "              f\"val_loss={val_loss:.4f}, \"\n",
    "              f\"time={elapsed:.2f}s, \"\n",
    "              f\"max_gpu_mem={max_mem:.1f} MB\")\n",
    "\n",
    "    return {\n",
    "        \"description\": desc,\n",
    "        \"total_params\": total,\n",
    "        \"trainable_params\": trainable,\n",
    "        \"history\": history,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8349c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Qwen full fine-tune ===\n",
      "Всего параметров:     494.03M\n",
      "Обучаемых параметров: 494.03M (100.000%)\n"
     ]
    }
   ],
   "source": [
    "def make_short_loader(loader, max_batches=200):\n",
    "    it = iter(loader)\n",
    "    for _ in range(max_batches):\n",
    "        try:\n",
    "            yield next(it)\n",
    "        except StopIteration:\n",
    "            return\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "result_full = run_experiment(base_model, make_short_loader(train_loader, max_batches=200), make_short_loader(valid_loader, max_batches=50), device=device, lr=1e-4, epochs=1, desc=\"Qwen full fine-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "apply_lora(lora_model, r=8, alpha=16, dropout=0.05)\n",
    "freeze_base_and_unfreeze_lora(lora_model)\n",
    "\n",
    "result_lora = run_experiment(lora_model, make_short_loader(train_loader, max_batches=200), make_short_loader(valid_loader, max_batches=50), device=device, lr=1e-3, epochs=1, desc=\"Qwen + LoRA (только A,B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220bd15-3681-4006-b7e0-44838b3500ad",
   "metadata": {},
   "source": [
    "### Supervised Fine-tuning\n",
    "\n",
    "__Задание 2 (3 балла).__ Разбейте все примеры с \"хорошими\" ответами на запросы (все что идет до последнего \"Assistant:\") и ответы (все, начиная с последнего \"Assistant:\"). Дообучите модель [`pythia-1.4b`](https://huggingface.co/EleutherAI/pythia-1.4b) генерировать правильные ответы с помощью вашей LoRA. Одной эпохи вполне должно хватить для сходимости. Проверьте на нескольких случайных тестовых примерах, что модель ведет себя так, как надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad95962-e8c1-42fd-91e6-9eee15072c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a11a0-cd78-4a6e-bc27-cb61c42f1a4f",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e921f-279e-46ae-8c70-5d715b91106e",
   "metadata": {},
   "source": [
    "__Задание 3 (3 балла).__ Реализуйте DPO согласно [статье](https://arxiv.org/pdf/2305.18290) и дообучите SFT модель с предыдущего шага. Одной эпохи так же должно хватить, но можно обучать и дольше. Убедитесь, что модель начинает отдавать предпочтение хорошим ответам. Проведите анализ. Стали ли ответы лучше, чем у SFT модели? Всегда ли модель отвечает хорошо или иногда плохо? Насколько легко модель ломается при изменении промптов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8129e5-b886-47dc-80ae-6267bf56181f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
