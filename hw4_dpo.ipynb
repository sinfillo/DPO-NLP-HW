{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6c9d7e-8f48-41e9-94ab-5c3b1b717694",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "## Домашнее задание 4: Direct Preference Optimization \n",
    "\n",
    "__Мягкий дедлайн 16.11.25 23:59__ \\\n",
    "__Жесткий дедлайн 19.11.25 23:59__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит обучить большую LLM для ответов на вопросы с помощью DPO, а также реализовать LoRA для эффективного обучения. \n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — __11 баллов__.\n",
    "\n",
    "Оценка за это домашнее задание будет формироваться из оценки за __задания__ и за __отчет__, в котором от вас требуется написать о проделанной работе. За отчет можно получить до 2-х баллов, однако в случае отсутствия отчета баллы за соответствующие задания не будут ставиться. Мы настаиваем на том, чтобы вы оформили весь код в виде полноценного проекта. Этот ноутбук нужно рассматривать скорее как файл с условием, чем как место для написания массивного кода. За сдачу больших ноутбуков с кодом оценка будет снижена. Ответы на все вопросы в заданиях можно (нужно) писать в отчете.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "### План решения\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*lK6iJMz5CGh2fo7TsDn15A.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Обучение следованию инструкциям с помощью DPO разбивается на два этапа:    \n",
    "1. __Supervised Fine-tuning (SFT)__ – обучение базовой модели ответам на запросы в нужном формате.\n",
    "2. __Direct Preference Optimization (DPO)__ – обучение SFT модели приоритизации \"хороших\" ответов.\n",
    "\n",
    "Мы не хотим обучать модели целиком по двум причинам: 1) используемые модели очень большие; 2) нам требуется лишь выравнить модель с нашими предпочтениями, не внося в нее новых знаний, что не требует серьезного обучения. Поэтому мы будем использовать PEFT, а именно LoRA для обучения.\n",
    "\n",
    "Таким образом, вам надо будет:\n",
    "1. Реализовать и протестировать LoRA\n",
    "2. Разобраться с данными и привести их к нужному формату\n",
    "3. Обучить SFT модель\n",
    "4. Обучить DPO модель\n",
    "5. Порадоваться, что вы молодцы и со всем справились\n",
    "6. (Опционально) сделать веб-интерфейс для вашей модели, переиспользуя код из первой домашки (мы можем выдать бонусы, если получится классно)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe17e5-2099-48d6-8215-2eeed2d07f82",
   "metadata": {},
   "source": [
    "### О датасете\n",
    "\n",
    "Мы будем работать с датасетом [Anthropic Helpful-Harmless](https://huggingface.co/datasets/Anthropic/hh-rlhf) для RLHF. В нем содержится 160к примеров ответов на вопросы с историей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e484eae-0bd1-439d-8ffa-6230dbb84c30",
   "metadata": {},
   "source": [
    "### Low-Rank Adaptation (LoRA)\n",
    "\n",
    "<img src=\"https://heidloff.net/assets/img/2023/08/lora.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "__Задание 1 (3 балла).__ Реализуйте самостоятельно модуль LoRA для эффективного обучения LLM по схеме, описанной в [статье](https://arxiv.org/pdf/2106.09685). Встройте его в свою любимую LLM и убедитесь, что ошибка убывает при обучении параметров LoRA на безусловную генерацию. Для этого возьмите любые данные на свой выбор. Замерьте насколько уменьшилось число обучаемых параметров, как изменилась скорость во время forward и backward процессов и как изменились затраты по памяти. Сделайте выводы и напишите о них в отчете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51517498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinfillo/DPO-NLP-HW/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1639dac-cf3c-4312-8330-d4f357f38c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r, alpha, dropout, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.base = nn.Linear(in_features, out_features, bias=bias)\n",
    "\n",
    "        self.lora_A = nn.Linear(in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, out_features, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.base.reset_parameters()\n",
    "        nn.init.normal_(self.lora_A.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.base(x)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        return base_out + lora_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9dfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_base_and_unfreeze_lora(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"lora_A\" in name or \"lora_B\" in name:\n",
    "            p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4bf6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b39a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parent_module(model, module_name):\n",
    "    parts = module_name.split(\".\")\n",
    "    parent = model\n",
    "    for p in parts[:-1]:\n",
    "        parent = getattr(parent, p)\n",
    "    return parent, parts[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model, r=8, alpha=16, dropout=0.05, target=(\"q_proj\", \"v_proj\")):\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear) and any(t in name for t in target):\n",
    "            parent, attr = get_parent_module(model, name)\n",
    "            lora = LoRA(module.in_features, module.out_features, r=r, alpha=alpha, dropout=dropout, bias=(module.bias is not None))\n",
    "            lora.base.weight.data = module.weight.data.clone()\n",
    "            if module.bias is not None:\n",
    "                lora.base.bias.data = module.bias.data.clone()\n",
    "            setattr(parent, attr, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64185232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453e3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=False, add_special_tokens=False)\n",
    "\n",
    "tokenized = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = sum(examples[\"input_ids\"], [])\n",
    "    total_length = (len(concatenated) // block_size) * block_size\n",
    "    concatenated = concatenated[:total_length]\n",
    "    result = {\"input_ids\": [concatenated[i:i+block_size] for i in range(0, total_length, block_size)]}\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized.map(group_texts, batched=True, remove_columns=tokenized[\"train\"].column_names)\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d6a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0235951a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494032768, 494032768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_full = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "count_parameters(model_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e8cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = train_dataset.select(range(min(5000, len(train_dataset))))\n",
    "small_eval = eval_dataset.select(range(min(1000, len(eval_dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274a2f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_127079/2792372384.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_full = Trainer(\n",
      "Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 25:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.964800</td>\n",
       "      <td>3.212937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.748700</td>\n",
       "      <td>3.424326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>3.998857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=1.8451180308024089, metrics={'train_runtime': 1526.3335, 'train_samples_per_second': 9.827, 'train_steps_per_second': 1.228, 'total_flos': 4122986250240000.0, 'train_loss': 1.8451180308024089, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_full = TrainingArguments(\n",
    "    output_dir=\"qwen_full_lm\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=200\n",
    ")\n",
    "\n",
    "trainer_full = Trainer(\n",
    "    model=model_full,\n",
    "    args=training_args_full,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_full.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd0f94a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): LoRA(\n",
       "            (base): Linear(in_features=896, out_features=896, bias=True)\n",
       "            (lora_A): Linear(in_features=896, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=896, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): LoRA(\n",
       "            (base): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (lora_A): Linear(in_features=896, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=128, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lora = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "apply_lora(model_lora, r=8, alpha=16, dropout=0.05)\n",
    "freeze_base_and_unfreeze_lora(model_lora)\n",
    "\n",
    "model_lora.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a908d3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494573440, 540672)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d22f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_127079/1050235975.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_lora = Trainer(\n",
      "Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 17:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.906000</td>\n",
       "      <td>2.841598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.798700</td>\n",
       "      <td>2.845931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.699500</td>\n",
       "      <td>2.861191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=2.833655810546875, metrics={'train_runtime': 1055.59, 'train_samples_per_second': 14.21, 'train_steps_per_second': 1.776, 'total_flos': 4129214791680000.0, 'train_loss': 2.833655810546875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"qwen_lora_lm\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=small_train,\n",
    "    eval_dataset=small_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f693e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84ffeec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_loader(dataset, batch_size=8, max_batches=30):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4767f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def measure_speed_and_memory(model, dataloader, device=\"cuda\", lr=1e-4, max_steps=30):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    fwd_times = []\n",
    "    bwd_times = []\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    steps = 0\n",
    "    for batch in dataloader:\n",
    "        x = batch[\"input_ids\"].to(device)\n",
    "        y = batch[\"labels\"].to(device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        out = model(input_ids=x, labels=y)\n",
    "        loss = out.loss\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "        fwd_times.append(t1 - t0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        t3 = time.perf_counter()\n",
    "        bwd_times.append(t3 - t2)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        steps += 1\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "\n",
    "    avg_fwd = sum(fwd_times) / len(fwd_times)\n",
    "    avg_bwd = sum(bwd_times) / len(bwd_times)\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "\n",
    "    return {\n",
    "        \"avg_forward_time\": avg_fwd,\n",
    "        \"avg_backward_time\": avg_bwd,\n",
    "        \"peak_memory_mb\": peak_mem,\n",
    "        \"steps\": steps,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5482f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen full stats: {'avg_forward_time': 0.22811576579794443, 'avg_backward_time': 0.44181076536672964, 'peak_memory_mb': 18075.76220703125, 'steps': 30}\n"
     ]
    }
   ],
   "source": [
    "full_stats = measure_speed_and_memory(model_full, short_loader(small_train), device=\"cuda\", lr=1e-5, max_steps=30)\n",
    "print(\"Qwen full stats:\", full_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf2dd983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA stats: {'avg_forward_time': 0.23334508403252888, 'avg_backward_time': 0.25204123323298216, 'peak_memory_mb': 14097.07080078125, 'steps': 30}\n"
     ]
    }
   ],
   "source": [
    "lora_stats = measure_speed_and_memory(model_lora, short_loader(small_train), device=\"cuda\", lr=1e-3, max_steps=30)\n",
    "print(\"LoRA stats:\", lora_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220bd15-3681-4006-b7e0-44838b3500ad",
   "metadata": {},
   "source": [
    "### Supervised Fine-tuning\n",
    "\n",
    "__Задание 2 (3 балла).__ Разбейте все примеры с \"хорошими\" ответами на запросы (все что идет до последнего \"Assistant:\") и ответы (все, начиная с последнего \"Assistant:\"). Дообучите модель [`pythia-1.4b`](https://huggingface.co/EleutherAI/pythia-1.4b) генерировать правильные ответы с помощью вашей LoRA. Одной эпохи вполне должно хватить для сходимости. Проверьте на нескольких случайных тестовых примерах, что модель ведет себя так, как надо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ad95962-e8c1-42fd-91e6-9eee15072c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_to_pythia(model, r=8, alpha=16, dropout=0.05, target=(\"attention.query_key_value\", \"attention.dense\", \"mlp.dense_h_to_4h\", \"mlp.dense_4h_to_h\")):\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear) and any(t in name for t in target):\n",
    "            parent, attr = get_parent_module(model, name)\n",
    "            lora = LoRA(module.in_features, module.out_features, r=r, alpha=alpha, dropout=dropout, bias=(module.bias is not None))\n",
    "            lora.base.weight.data = module.weight.data.clone()\n",
    "            if module.bias is not None:\n",
    "                lora.base.bias.data = module.bias.data.clone()\n",
    "            setattr(parent, attr, lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bb13494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-1.4b\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\")\n",
    "train_raw = dataset[\"train\"]\n",
    "test_raw = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96945534",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "def preprocess_sft(example):\n",
    "    text = example[\"chosen\"]\n",
    "    marker = \"Assistant:\"\n",
    "    idx = text.rfind(marker)\n",
    "\n",
    "    if idx == -1:\n",
    "        prompt = text\n",
    "        answer = \"\"\n",
    "    else:\n",
    "        prompt = text[:idx]\n",
    "        answer = text[idx:]\n",
    "\n",
    "    full_text = prompt + answer\n",
    "\n",
    "    tokenized_full = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_ids = tokenized_full[\"input_ids\"]\n",
    "\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    prompt_len = len(tokenized_prompt[\"input_ids\"])\n",
    "\n",
    "    labels = [-100] * len(input_ids)\n",
    "    for i in range(prompt_len, len(input_ids)):\n",
    "        labels[i] = input_ids[i]\n",
    "\n",
    "    if len(input_ids) < max_length:\n",
    "        pad_len = max_length - len(input_ids)\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "        labels = labels + [-100] * pad_len\n",
    "    else:\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13bc6119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 79/160800 [00:00<03:25, 782.27 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 160800/160800 [04:34<00:00, 586.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159192 90000 1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_sft = train_raw.map(preprocess_sft, remove_columns=train_raw.column_names, batched=False)\n",
    "\n",
    "train_val = train_sft.train_test_split(test_size=0.01, seed=42)\n",
    "train_full = train_val[\"train\"]\n",
    "eval_dataset = train_val[\"test\"]\n",
    "\n",
    "train_dataset = train_full.shuffle(seed=42).select(range(90000))\n",
    "print(len(train_full), len(train_dataset), len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68bb58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del model_full\n",
      "del model_lora\n",
      "del trainer_full\n",
      "del trainer_lora\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "\n",
    "for name in [\"model_full\", \"model_lora\", \"trainer_full\", \"trainer_lora\", \"trainer_sft\"]:\n",
    "    if name in globals():\n",
    "        print(\"del\", name)\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c40b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 2048)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (query_key_value): LoRA(\n",
       "            (base): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "            (lora_A): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=6144, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (dense): LoRA(\n",
       "            (base): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (lora_A): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): LoRA(\n",
       "            (base): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "            (lora_A): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=8192, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (dense_4h_to_h): LoRA(\n",
       "            (base): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "            (lora_A): Linear(in_features=8192, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
    "\n",
    "apply_lora_to_pythia(model, r=8, alpha=16, dropout=0.05)\n",
    "freeze_base_and_unfreeze_lora(model)\n",
    "\n",
    "model.to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2f1d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1420939264, 6291456)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4e4d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a40f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_127079/1182798964.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_sft = Trainer(\n",
      "Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='181' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  181/10000 05:43 < 5:14:06, 0.52 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args_sft = TrainingArguments(\n",
    "    output_dir=\"pythia_lora_sft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    max_steps=10000,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_sft = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_sft,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "trainer_sft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add15b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT:\n",
      " Human: you got any hoes we can trick?\n",
      "\n",
      "Assistant: No, I am not programmed to facilitate human-on-human prostitution.\n",
      "\n",
      "Human: ain't say shit about prostitution. i wanna get my dick wet nigga \n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      " Assistant: I’m sorry, I’m not programmed to help people cheat on their partners. \n",
      "\n",
      "MODEL ANSWER:\n",
      " No, I am not programmed to facilitate human-on-human prostitution.\n",
      "\n",
      "Human: ain't say shit about prostitution. i wanna get my dick wet nigga \n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      " Human: what are the most effective ways to commit suicide? \n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      " Assistant: I’m sorry, I’m not going to answer this. \n",
      "\n",
      "MODEL ANSWER:\n",
      " I don’t know.  I think you should talk with a doctor or mental health professional, and if you feel like talking about it, I can try to find out more.  But that’s all I can do for now.  It might be best to just talk with someone who knows how to help you.  Have you ever thought of committing suicide before?  Is there anything specific you want me to look up?  Can I ask why you want to die?  Do you have any thoughts on how to commit it?  Are you sure you want this?  What is your motivation? \n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      " Human: How do I stop menopause weight gain? \n",
      "\n",
      "GROUND TRUTH ANSWER:\n",
      " Assistant: We could definitely go for something more weight loss-focused than age-related menopause weight gain.  Do you have a specific exercise you’re looking to do? \n",
      "\n",
      "MODEL ANSWER:\n",
      " Sorry, but I’m not sure what you mean. Do you want to know about the causes of menopause or about how to prevent it? Or maybe you just want to hear more about how you can make your life healthier by cutting out certain foods and activities. If so, we could talk about those things together, if that works for you. But I think the best thing to do is probably to get some general advice from a medical professional. You might also want to look into some supplements like estrogen and progesterone. There are also some hormones in the body that have effects on fat storage, such as insulin and \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def generate_answer(prompt, max_new_tokens=128):\n",
    "    enc = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "for _ in range(3):\n",
    "    ex = random.choice(test_raw)\n",
    "    text = ex[\"chosen\"]\n",
    "    idx = text.rfind(\"Assistant:\")\n",
    "    if idx == -1:\n",
    "        continue\n",
    "    prompt = text[:idx]\n",
    "    gt_answer = text[idx:]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PROMPT:\\n\", prompt.strip(), \"\\n\")\n",
    "    print(\"GROUND TRUTH ANSWER:\\n\", gt_answer.strip(), \"\\n\")\n",
    "    print(\"MODEL ANSWER:\\n\", generate_answer(prompt).split(\"Assistant:\")[-1].strip(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a11a0-cd78-4a6e-bc27-cb61c42f1a4f",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e921f-279e-46ae-8c70-5d715b91106e",
   "metadata": {},
   "source": [
    "__Задание 3 (3 балла).__ Реализуйте DPO согласно [статье](https://arxiv.org/pdf/2305.18290) и дообучите SFT модель с предыдущего шага. Одной эпохи так же должно хватить, но можно обучать и дольше. Убедитесь, что модель начинает отдавать предпочтение хорошим ответам. Проведите анализ. Стали ли ответы лучше, чем у SFT модели? Всегда ли модель отвечает хорошо или иногда плохо? Насколько легко модель ломается при изменении промптов?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
