# Отчет

Для эксперимента в ДЗ я выбрала модель Qwen2-0.5B -- компактную, но всё же достаточно крупную LLM, чтобы разница между полным fine-tuning и LoRA была хорошо заметна по параметрам, скорости и потреблению памяти.

Класс `LoRA` -- реализует LoRA по статье. К матрицe W мы добавляем низкоранговый сдвиг AB. В конечном виде слой получился с замороженной базовой матрицей и двумя обучаемыми матрицами A и B (инициализируем A -- случайно, B -- нулями). Отдельно пришлось добавить scaling-фактор и dropout для LoRA-ветки, иначе модель становилась нестабильной на первых шагах обучения. 

Следующая сложность заключалась во встраивании LoRA в Qwen. У модели довольно странная структура с множеством вложенных модулей, поэтому я написала вспомогательную функцию `get_parent_module`, которая по имени слоя находила родителя, чтобы корректно подменять `q_proj` и `v_proj` на мой класс `LoRA`. 

После замены слоёв я заморозила все параметры модели и разморозила только веса A и B в LoRA модулях, то есть обучаемая часть модели уменьшилась примерно в 70–100 раз. Тогда я подготовила данные для безусловной генерации -- использовала Wikitext-2 и порезала его на блоки фиксированной длины. Тут тоже пришлось столкнуться с мелочами: 
- токенизатор Qwen требует, чтобы pad токен совпадал с eos токеном, иначе он выдает предупреждение и может как-то плохо себя вести
- если брать весь датасет целиком, обучение full fine-tuning требует слишком много времени и памяти, поэтому я ограничила количество выборки для сравнения двух режимов.
Далее я построила одинаковый тренировочный цикл для обоих вариантов.

После этого я запустила оба режима -- полный fine-tuning и обучение только LoRA-параметров -- все это на одинаковом количестве батчей. В полном fine-tuning модель оптимизировала все свои параметры, и даже при сравнительно небольшом количестве шагов становилось видно, насколько тяжёлым получается backward: GPU забивается почти полностью, а скорость падает примерно вдвое по сравнению с LoRA-вариантом. В LoRA же, наоборот, обучались лишь матрицы A и B, и из-за этого backward оказался значительно быстрее и требовал гораздо меньше видеопамяти.

При этом, что приятно, в обоих режимах ошибка уверенно убывала. В full fine-tuning снижение было чуть более агрессивным (это ожидаемо, ведь модель фактически переобучается целиком). Однако LoRA почти не отставала: начальная ошибка была выше, но после нескольких десятков шагов лосс стабилизировался. Значит LoRA корректно встроена и не нарушила логику вычислений внутри attention.

После того, как первая часть работала стабильно, я перешла к supervised fine-tuning. Здесь модель обучалась уже не просто предсказывать следующий токен, а генерировать содержательные ответы на инструкции. Этот этап потребовал заменить лишь датасет и функцию подготовки батчей, сам тренировочный цикл и LoRA-интеграция остались теми же. SFT шёл без сюрпризов: модель быстро начинала повторять стиль ответов из обучающей выборки, а loss убывал так же плавно, как в LM. Я отдельно проверяла, что LoRA-параметры действительно обновляются, а базовая часть модели остаётся замороженной, -- это тоже работало корректно. Только качество сгенерированных ответов мне совсем не понравилось, но думааю, что это пофиксилось бы DPO.